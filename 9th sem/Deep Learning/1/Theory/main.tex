\documentclass{article}

\usepackage{graphicx}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{float}
\usepackage{neuralnetwork}
\usepackage[sorting=none]{biblatex}
\usepackage[margin=1in]{geometry}
\usepackage[font={small,it}]{caption}
\usepackage{placeins}
\usepackage{xepersian}

%\DeclareMathOperator*{\btie}{\bowtie}
\addbibresource{bibliography.bib}
\settextfont[Scale=1.2]{B-NAZANIN.TTF}
\setlatintextfont[Scale=1]{Times New Roman}
\renewcommand{\baselinestretch}{1.5}
\pagestyle{fancy}
\fancyhf{}
\rhead{تکلیف اول درس یادگیری عمیق}
\lhead{\thepage}
\rfoot{علیرضا ابره فروش}
\lfoot{9816603}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
%%%%%%%%%%
\lstset
{
    language=[latex]tex,
    basicstyle=\ttfamily,
    commentstyle=\color{black},
    columns=fullflexible,
    keepspaces=true,
    upquote=true,
    showstringspaces=false,
    morestring=[s]\\\%,
    stringstyle=\color{black},
}
%%%%%%%%%%
%beginMatlab
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
%endMatlab
\begin{document}
%beginMatlab
\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}
%endMatlab
\input{titlepage}

%\tableofcontents
\newpage


%1
\section{}
%$
%m = (00110010)_2 = (50)_{10} \\
%p = 23, q = 19 \\
%n = pq = 23 \times 19 = 437 \\
%z = (p-1)(q-1) = 22 \times 18 = 396 \\
%e = 97 \Rightarrow  e < n,\: (e, z) = 1 \\
%d = 49 \Rightarrow ed \equiv 1 (mod\:z) \\ \\
%c = (m ^ e \equiv 1 (mod \: n)) \Rightarrow c = 335 \\
%m = (c ^ d \equiv 1 (mod \: n)) \Rightarrow m = 50 \\ \\
%K^+ = e = 97 \\
%K^- = d = 49 \\
%$


%2
\section{}
%\lr{Trudy} می‌تواند به سیستم نفوذ کند و محتوای ارسال شده را تغییر دهد. به این صورت که $M$ و $H(M) + S$ را از هم جدا می‌کند. سپس با محاسبه‌ی \lr{hash}ِ پیامِ $M$ و حذف آن از $H(M)+S$ (به شکل $H(M) + S - H(M)$) به \lr{shared secret} دست پیدا می‌کند. سپس پیام مورد نظر خود ($M^{\prime}$) را به همراه $H(M^{\prime})$ و $S$ برای فرد به شکل
%$\left( M^{\prime}, H(M^{\prime}) + S \right)$
%ارسال می‌کند. با اینکه این پروتکل قابلیت نفوذ دارد اما به هر حال نمودار استخراج و \lr{Authentication}ِ پیام به صورت زیر است.
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.50\textwidth]{figures/1.png}
%    \caption
%	{
%نمودار رمزگشایی پیام
%	}
%    \label{fig:fig1}
%\end{figure}


%3
\section{}
\subsection{الف}
تابع تانژانت هایپربولیک ($tanh$) اغلب به عنوان نسخه مقیاس‌شده‌ای از تابع سیگموید توصیف می‌شود، به خصوص تابع سیگموید لجستیک. این رابطه به دلیل شباهت‌های تابع تانژانت و تابع سیگموید وجود دارد، اما در بازه و مقیاس‌شان تفاوت دارند.\\
تابع سیگموید که اغلب با نماد $\sigma\left( x \right)$ نشان داده می‌شود، به شرح زیر تعریف می‌شود:
\begin{latin}
$
\sigma\left( x \right) = \frac{1}{1 + e ^ {-x}}
$
\end{latin}
این تابع هر عدد حقیقی را به یک مقدار بین 0 و 1 نگاشت می‌کند. وقتی $x$ یک عدد مثبت بزرگ است، $\sigma\left( x \right)$ به 1 نزدیک می‌شود و وقتی $x$ یک عدد منفی بزرگ است، $\sigma\left( x \right)$ به 0 نزدیک می‌شود. این به این معناست که تابع سیگموید ورودی خود را در بازه (0، 1) فشرده می‌کند که برای مسائل دسته‌بندی دودویی مفید است، چون می‌توان از آن تعبیر احتمالاتی کرد.\\
تابع تانژانت هایپربولیک، به شکل زیر تعریف می‌شود:
\begin{latin}
$
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e ^ {2x} - 1}{e ^ {2x} + 1}
$
\end{latin}
تابع تانژانت هایپربولیک هر عدد حقیقی را به یک مقدار بین 1- و 1 نگاشت می‌دهد. وقتی $x$ یک عدد مثبت بزرگ است، $tanh(x)$ به 1 نزدیک می‌شود و وقتی $x$ یک عدد منفی بزرگ است، $tanh(x)$ به 1- نزدیک می‌شود.\\

رابطه بین توابع تانژانت هایپربولیک و سیگموید به شرح زیر است:
\begin{enumerate}
\item مقیاس‌دهی: تابع تانژانت هایپربولیک، انتقال داده شده و مقیاس شده‌ی تابع سیگموید به منظور داشتن رنج $(-1, 1)$ به جای $(0, 1)$ است. این مقیاس‌دهی با کم کردن $0.5$ از تابع سیگموید و سپس ضرب آن در 2 انجام می‌شود:
\begin{latin}
$
2\sigma\left( 2x \right)-1 = 
2 \times \frac{1}{1 + e ^ {-2x}} - 1 =
\frac{1 - e ^ {-2x}}{1 + e ^ {-2x}} = 
\frac{1 - e ^ {-2x}}{1 + e ^ {-2x}} \times \frac{e ^ {2x}}{e ^ {2x}} =
\frac{e ^ {2x} - 1}{e ^ {2x} + 1} = tanh\left( x \right)\\ \Rightarrow 
tanh(x) = 2\sigma\left( 2x \right) - 1
$
\end{latin}

\item تقارن: یکی از تفاوت‌های کلیدی این است که تابع تانژانت هایپربولیک در اطراف مبدأ متقارن است (در واقع $tanh(-x) = -tanh(x)$)، درحالی که تابع سیگموید این تقارن را ندارد.

\end{enumerate}
تابع تانژانت هایپربولیک اغلب به عنوان یک جایگزین برای تابع سیگموید در شبکه‌های عصبی استفاده می‌شود. زیرا به دلیل ماهیت مرکز شده حول صفری که دارد، یادگیری در شبکه‌های عمیق را سریع‌تر می‌کند. اینکه تابع تانژانت هایپربولیک مقادیر منفی را نیز خروجی دهد به معنای این است که می‌تواند تغییرات مثبت و منفی را در واحدهای مخفی شبکه در طول آموزش ایجاد کند که می‌تواند به همگرایی کمک کند. با این حال، هر دو تابع هنوز در متنوعی از زمینه‌ها استفاده می‌شوند و انتخاب بین آن‌ها بستگی به مسئله خاص و معماری شبکه دارد.



\subsection{ب}
\begin{latin}
$
p\left( x \right) = x \log\left( 1 + tanh\left( e ^ {x} \right) \right)\\ \\
\frac{d}{dx} p\left( x \right) = \left( \frac{d}{dx}x \right) \times \log\left( 1 + tanh\left( e ^ {x} \right) \right) +
\left( \frac{d}{dx}\left( \log\left( 1 + tanh\left( e ^ {x} \right) \right) \right) \right) \times x \\=
\log\left( 1 + tanh\left( e ^ {x} \right) \right) + 
x\left( \frac{d }{dx}\left( 1 + tanh\left( e ^ {x} \right) \right) \right) \times \frac{1}{1 + tanh\left( e ^ {x} \right)}
\\= \log\left( 1 + tanh\left( e ^ {x} \right) \right) +
\frac{x}{1 + tanh\left( e ^ {x} \right)} \times \left( \frac{d }{dx}\left( tanh\left( e ^ {x} \right) \right) \right)
\\= \log\left( 1 + tanh\left( e ^ {x} \right) \right) + \frac{x}{1 + tanh\left( e ^ {x} \right)} \times 
\left( \frac{d }{dx}\left( e ^ {x} \right) \right) \times \left( 1 - tanh^2\left( e ^ x \right) \right)
\\=
\log\left( 1 + tanh\left( e ^ {x} \right) \right) + 
xe^x\left( 1 - tanh\left( e ^ x \right) \right)
$
\end{latin}

%4
\section{}
\begin{latin}
\begin{neuralnetwork}[height=9]
    \newcommand{\x}[2]{$x_#2$}
    \newcommand{\y}[2]{$\hat{y}_#2$}
    \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
    \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
    \inputlayer[count=4, bias=true, title=Input\\layer, text=\x]
    \hiddenlayer[count=7, bias=true, title=Hidden\\layer, text=\hfirst] \linklayers
    \outputlayer[count=3, title=Output\\layer, text=\y] \linklayers
\end{neuralnetwork}
\end{latin}
با توجه به شبکه‌ی بالا، نورون‌های سبز، بنفش، و قرمز به ترتیب لایه‌ی ورودی، لایه‌ی مخفی، و لایه‌ی خروجی را تشکیل می‌دهند و همچنین نورون‌های زرد \lr{bias}ها هستند که همگی مقدار 1 دارند. پارامترهای قابل یادگیری شبکه وزن‌های موجود بین نورون‌هاست که تعدادشان برابر است با:
$
4 \times 7 + 7 + 7 \times 3 + 3 = 59
$


%5
\section{}

\begin{latin}
\begin{neuralnetwork}[height=4, layerspacing=20mm, nodespacing=15mm, maintitleheight=2.5em, layertitleheight=2.5em]
    \newcommand{\x}[2]{$x_#2$}
    \newcommand{\relu}[2]{\small $a_#2$}
    \newcommand{\y}[2]{$\hat{y}$}
    \inputlayer[count=2, bias=false, title=Input Layer, text=\x]
    \hiddenlayer[count=3, bias=false, title=ReLU Layer, text=\relu] \linklayers
    \outputlayer[count=1, title=Output Layer, text=\y] \linklayers
\end{neuralnetwork}
\end{latin}

\begin{latin}
$
J\left( W \right) = \frac{1}{n} \sum_{i = 1}^{n} L\left( \hat{y} ^ {( i )}, y ^ { ( i ) } \right) = \left( \hat{y} ^ {( 1 )} - y ^ {( 1 )} \right) ^ {2} = \left( \hat{y} - 3 \right) ^ {2}
\\
x = \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\\
z = \begin{bmatrix}
z_1 \\
z_2 \\
z_3
\end{bmatrix} =
\begin{bmatrix}
w_1 & w_2 \\
w_3 & w_4 \\
w_5 & w_6
\end{bmatrix}
x
\\
a = \begin{bmatrix}
a_1 \\
a_2 \\
a_3
\end{bmatrix} =
ReLU\left( z \right)
\\
z_4 =
\begin{bmatrix}
w_7 &
w_8 &
w_9
\end{bmatrix}a
\\
\hat{y} = ReLU\left( z_4 \right)
\\
\\
\\
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}=
\begin{bmatrix}
1 \\
2
\end{bmatrix}
\\ y = 3
\\
\begin{bmatrix}
z_1 \\
z_2 \\
z_3
\end{bmatrix} =
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
\begin{bmatrix}
1 \\
2
\end{bmatrix} =
\begin{bmatrix}
4 \\
-3 \\
5
\end{bmatrix}
\\ a = 
ReLU\left(
\begin{bmatrix}
4 \\
-3 \\
5
\end{bmatrix}
\right) =
\begin{bmatrix}
4 \\
0 \\
5
\end{bmatrix}
\\
z_4 = 
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
\begin{bmatrix}
4 \\
0 \\
5
\end{bmatrix} = 6
\\
\hat{y} = ReLU\left( 6 \right) = 6
\\
\\
\\
\begin{bmatrix}
w_7 &
w_8 &
w_9
\end{bmatrix}
\gets 
\begin{bmatrix}
w_7 &
w_8 &
w_9
\end{bmatrix}
-\alpha
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial w_7} &
\frac{\partial J\left( W \right)}{\partial w_8} &
\frac{\partial J\left( W \right)}{\partial w_9}
\end{bmatrix} 
\\ =
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
-0.1
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_7} &
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_8} &
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_9}
\end{bmatrix}
\\=
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
-0.1
\begin{bmatrix}
2\left( \hat{y} - y \right)a_1 &
2\left( \hat{y} - y \right)a_2 &
2\left( \hat{y} - y \right)a_3
\end{bmatrix}
\\=
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
-0.1
\begin{bmatrix}
24 &
0 &
30
\end{bmatrix} = \begin{bmatrix}
-3.4 &
3 &
-1
\end{bmatrix}
\\ \\ 
\begin{bmatrix}
w_1 & w_2 \\
w_3 & w_4 \\
w_5 & w_6
\end{bmatrix}
\gets 
\begin{bmatrix}
w_1 & w_2 \\
w_3 & w_4 \\
w_5 & w_6
\end{bmatrix}
-\alpha
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial w_1} & \frac{\partial J\left( W \right)}{\partial w_2} \\
\frac{\partial J\left( W \right)}{\partial w_3} & \frac{\partial J\left( W \right)}{\partial w_4} \\
\frac{\partial J\left( W \right)}{\partial w_5} & \frac{\partial J\left( W \right)}{\partial w_6}
\end{bmatrix}
\\=
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
-0.1
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}
&
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_2}
\\
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_2} \times \frac{\partial a_2}{\partial z_2} \times \frac{\partial z_2}{\partial w_3}
&
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_2} \times \frac{\partial a_2}{\partial z_2} \times \frac{\partial z_2}{\partial w_4}
\\
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_3} \times \frac{\partial a_3}{\partial z_3} \times \frac{\partial z_3}{\partial w_5}
&
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_3} \times \frac{\partial a_3}{\partial z_3} \times \frac{\partial z_3}{\partial w_6}
\end{bmatrix}
\\=
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
-0.1
\begin{bmatrix}
2\left( \hat{y} - y \right) \times w_7 \times 1 \times x_1
&
2\left( \hat{y} - y \right) \times w_7 \times 1 \times x_2
\\
2\left( \hat{y} - y \right) \times w_8 \times 0 \times x_1
&
2\left( \hat{y} - y \right) \times w_8 \times 0 \times x_2
\\
2\left( \hat{y} - y \right) \times w_9 \times 1 \times x_1
&
2\left( \hat{y} - y \right) \times w_9 \times 1 \times x_2
\end{bmatrix}
\\=
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
-0.1
\begin{bmatrix}
-6
&
-12
\\
0
&
0
\\
12
&
36
\end{bmatrix} =
\begin{bmatrix}
2.6 & 2.2 \\
1 & -2 \\
-0.2 & -1.6
\end{bmatrix}
$
\end{latin}


%6
\section{}


%\begin{enumerate}
%	\item \lr{Bob} لیستی از الگوریتم‌هایی که از آن‌ها پشتیبانی می‌کند را به همراه \lr{nonce} به \lr{Alice} می‌فرستد.
%	\item \lr{Alice} بین لیست الگوریتم‌های پیشنهادی یکی را انتخاب می‌کند و آن را به همراه \lr{certificate} و \lr{nonce} می‌فرستد.
%	\item \lr{Bob} \lr{certificate} را اعتبارسنجی می‌کند، \lr{public key}ِ \lr{Alice} را استخراج می‌کند، \lr{pre\_master\_secret} را تولید می‌کند، با \lr{public key}ِ \lr{Alice} آن را رمز می‌کند و برای \lr{Alice} می‌فرستد.
%	\item \lr{Bob} و \lr{Alice} به طور مستقل رمز و کلیدهای \lr{MAC} را از \lr{pre\_master\_secret} و \lr{nonce}ها محاسبه می‌کنند.
%	\item \lr{Bob} یک \lr{MAC} از همه‌ی پیام‌های \lr{handshake} می‌فرستد.
%	\item \lr{Alice} یک \lr{MAC} از همه‌ی پیام‌های \lr{handshake} می‌فرستد.
%\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section*{منابع}
\renewcommand{\section}[2]{}%
\begin{thebibliography}{99} % assumes less than 100 references
%چنانچه مرجع فارسی نیز داشته باشید باید دستور فوق را فعال کنید و مراجع فارسی خود را بعد از این دستور وارد کنید


\begin{LTRitems}

\resetlatinfont

\bibitem{b1} Stošić, Lazar, and Milena Bogdanović. "RC4 stream cipher and possible attacks on WEP." Editorial Preface 3.3 (2012).
\end{LTRitems}

\end{thebibliography}


\end{document}
