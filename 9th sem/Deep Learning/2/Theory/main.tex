\documentclass{article}

\usepackage{graphicx}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{textcomp}
\usepackage{float}
\usepackage{neuralnetwork}
\usepackage{pgfplots}
\usepackage[sorting=none]{biblatex}
\usepackage[margin=1in]{geometry}
\usepackage[font={small,it}]{caption}
\usepackage{placeins}
\usepackage{xepersian}

\pgfplotsset{width=8cm,compat=1.17}

%\DeclareMathOperator*{\btie}{\bowtie}
\addbibresource{bibliography.bib}
\settextfont[Scale=1.2]{B-NAZANIN.TTF}
\setlatintextfont[Scale=1]{Times New Roman}
\renewcommand{\baselinestretch}{1.5}
\pagestyle{fancy}
\fancyhf{}
\rhead{تکلیف دوم درس یادگیری عمیق}
\lhead{\thepage}
\rfoot{علیرضا ابره فروش}
\lfoot{9816603}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
%%%%%%%%%%
\lstset
{
    language=[latex]tex,
    basicstyle=\ttfamily,
    commentstyle=\color{black},
    columns=fullflexible,
    keepspaces=true,
    upquote=true,
    showstringspaces=false,
    morestring=[s]\\\%,
    stringstyle=\color{black},
}
%%%%%%%%%%
%beginMatlab
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
%endMatlab
\begin{document}
%beginMatlab
\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}
%endMatlab
\input{titlepage}

%\tableofcontents
\newpage


%1
\section{}
\begin{latin}
\subsection{Question:}
Prove that the lower bound for cross-entropy loss with $L$ correct classes is as following: \\
\begin{center}
$
L\left( \hat{y}, y \right) \ge L \log L, \quad y \in \left\{ 0, 1 \right\} ^ {n_y}
$
\end{center}
\subsection{Proof:}
Let's assume that $\hat{y_i}$'s are the outputs of the network. Without loss of generality, we assume $y_{k_1}, \cdots , y_{k_L}$ to be equal to 1 ($k_1, \cdots , k_L$ are correct classes). Considering that the sum of $\hat{y_i}$'s is equal to 1 and exactly $L$ of the $y_i$'s are equal to 1, we have:\\
\begin{center}
$
L\left( \hat{y}, y \right) = -\sum_{i = 1}^{n_y} y_i \log \hat{y}_i = -\sum_{j = 1}^{L} y_{k_j} \log \hat{y}_{k_j} = -\sum_{j = 1}^{L} \log \hat{y}_{k_j}
$
\end{center}
Considering AM-GM Inequality we have the following:\\
\begin{center}
$
\frac{\hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{L} \ge \sqrt[L]{\hat{y}_{k_1}\cdots\hat{y}_{k_L}}
$
\end{center}
$
\\ \\ \overset{\text{both sides to the power of $L$} }{\Longrightarrow} \\ \\
$
\begin{center}
$
\left( \frac{\hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{L} \right) ^ {L} \ge \hat{y}_{k_1}\cdots\hat{y}_{k_L}
$
\end{center}
$
\\ \\ \overset{1 \ge \hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{\Longrightarrow} \\ \\
$
\begin{center}
$
\left( \frac{1}{L} \right) ^ {L} \ge \left( \frac{\hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{L} \right) ^ {L} \ge \hat{y}_{k_1}\cdots\hat{y}_{k_L}
$
\end{center}
$
\\ \\ \overset{\text{log of each side}}{\Longrightarrow} \\ \\
$
\begin{center}
$
-L \log L \ge \log \left( \hat{y}_{k_1}\cdots\hat{y}_{k_L} \right) \Longrightarrow
$
\end{center}
\begin{center}
$
-\log \left( \hat{y}_{k_1}\cdots\hat{y}_{k_L} \right) \ge L \log L 
$
\end{center}
$
\\ \\ \Longrightarrow
$
\begin{center}
$
-\sum_{j = 1}^{L} \log \hat{y}_{k_j} \ge L \log L
$
\end{center}
$
\\ \\ \Longrightarrow
$
\begin{center}
$
L\left( \hat{y}, y \right) \ge L \log L
$
\end{center}
$
\qedsymbol{}
$
\end{latin}

\newpage
%2
\section{}
فرض کنیم که $\hat{y_i}$ها خروجی‌های شبکه باشند. بدون از دست رفتن کلیت فرض می‌کنیم
$y_{k_{1}}, \cdots, y_{k_{L}}$
برابر یک باشند ($k_1, \cdots, k_L$ کلاس‌های درست باشند). با توجه به اینکه مجموع $\hat{y_i}$ها برابر 1 است و دقیقا $L$تا از $y_i$ها برابر 1 هستند داریم:
\begin{latin}
$L\left( \hat{y}, y \right) = -\sum_{i = 1}^{n_y} y_i \log \hat{y}_i = -\sum_{j = 1}^{L} y_{k_j} \log \hat{y}_{k_j} = -\sum_{j = 1}^{L} \log \hat{y}_{k_j}$
\end{latin}
طبق نامساوی حسابی-هندسی داریم:
\begin{latin}
$
\frac{\hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{L} \ge \sqrt[L]{\hat{y}_{k_1}\cdots\hat{y}_{k_L}}
\\ \\ \overset{\text{both sides to the power of $L$} }{\Longrightarrow} \\ \\
\left( \frac{\hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{L} \right) ^ {L} \ge \hat{y}_{k_1}\cdots\hat{y}_{k_L}
\\ \\ \overset{1 \ge \hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{\Longrightarrow} \\ \\
\left( \frac{1}{L} \right) ^ {L} \ge \left( \frac{\hat{y}_{k_1} + \cdots + \hat{y}_{k_L}}{L} \right) ^ {L} \ge \hat{y}_{k_1}\cdots\hat{y}_{k_L}
\\ \\ \overset{\text{log of each side}}{\Longrightarrow} \\ \\
-L \log L \ge \log \left( \hat{y}_{k_1}\cdots\hat{y}_{k_L} \right) \Longrightarrow
-\log \left( \hat{y}_{k_1}\cdots\hat{y}_{k_L} \right) \ge L \log L 
\\ \\ \Longrightarrow
-\sum_{j = 1}^{L} \log \hat{y}_{k_j} \ge L \log L
\\ \\ \Longrightarrow
L\left( \hat{y}, y \right) \ge L \log L \\ \\
\qedsymbol{}
$
\end{latin}


%3
\section{}
با توجه به اینکه در انتهای نمودار \lr{Loss} در \lr{training} بسیار کمتر از \lr{validation} است، پس شبکه دچار \lr{overfitting} شده است.
\subsection{}
افزایش عمق شبکه عصبی می‌تواند به افزایش \lr{overfitting} منجر شود. شبکه‌های عمیق‌تر ظرفیت بالاتری برای یادگیری الگوهای پیچیده دارند، اما اگر مجموعه داده کافی بزرگ نباشد، \lr{overfitting} را تشدید خواهد کرد. اگر \lr{overfitting} ناشی از ظرفیت شبکه برای یادگیری نویز در داده‌های آموزش باشد، افزایش عمق ممکن است وضعیت را بدتر کند. با این حال، لازم به ذکر است اگر \lr{overfitting} به دلیل عدم یادگیری مدل از الگوهای موجود باشد، یک شبکه عمیق‌تر ممکن است به تعمیم بیشتر کمک کند.

\subsection{}
توقف زودهنگام یک تکنیک جهت جلوگیری از \lr{overfitting} است که شامل نظارت بر عملکرد مدل در یک مجموعه اعتبارسنجی و متوقف کردن فرآیند آموزش هنگامی که عملکرد شبکه بهبود چشمگیری نمی‌یابد یا شروع به افت می‌کند. این ممکن است در جلوگیری از \lr{overfitting} مؤثر باشد چرا که به مدل کمک می‌کند که خوب با داده‌های ناشناخته تعمیم پیدا کند. اگر یک مدل در حال \lr{overfitting} باشد، تکنیک توقف زودهنگام می‌تواند جلوی ادامه یادگیری نویز در داده‌های آموزش را بگیرد و بنابراین توانایی بهتری در تعمیم به داده‌های جدید برای آن ایجاد کند.
\subsection{}
آموزش بیشتر بدون هیچ گونه \lr{regularization} احتمالا \lr{overfitting} را تشدید می‌کند. ممکن است مدل به طور زیادی به داده‌های آموزش فیت شود و نتواند به داده‌های جدید و ناشناخته تعمیم پیدا کند. با این حال، اگر \lr{overfitting} ناشی از عدم یادگیری مدل از الگوهای موجود در داده‌ها باشد، آموزش بیشتر ممکن است به مدل کمک کند تا به یک وضعیت بهتر همگرا شود.
\subsection{}
\lr{Data Augmentation} شامل اعمال تبدیلات مختلف بر داده‌های آموزش، مانند چرخش‌ها، وارونه‌ها و جابجایی‌ها می‌شود. این می‌تواند به مدل کمک کند تا مقاومت بیشتری در برابر \lr{overfitting} پیدا کرده و با تغییرات در داده ورودی بهتر تعمیم پیدا کند. \lr{Data Augmentation} خصوصاً زمانی که مجموعه داده محدود است، می‌تواند با ارائه مدل به یک طیف گسترده‌تر از تغییرات در داده‌های آموزش، از \lr{overfitting} کاسته و توانایی بهتری در تعمیم ایجاد کند.


%4
\section{}
ابتدا تعریف می‌کنیم:
\begin{latin}
$
\forall i \in \left\{ 1, 2, \cdots, K \right\}: \:\:\:\: u_i := z^{[2]}_i \\ \\
A := \sum_{j = 1}^{K} e ^ {u_j}
$
\end{latin}

\subsection{الف}
\begin{latin}
$
\frac{\partial \hat{y}_k}{\partial z^{[2]}_k}
= \frac{\partial \hat{y}_k}{\partial u_k}
= \frac{\partial \left( \frac{e ^ {u_k}}{A} \right)}{\partial u_k}
= \frac{\frac{\partial e ^ {u_k}}{\partial u_k} \times A  - \frac{\partial A}{\partial u_k} \times e ^ {u_k}}{A ^ 2}
= \frac{e ^ {u_k}}{A} - \left( \frac{e ^ {u_k}}{A} \right) ^ {2}
= \hat{y}_k - \hat{y} ^ 2_k 
$
\end{latin}

\subsection{ب}
\begin{latin}
$
\frac{\partial \hat{y}_k}{\partial z^{[2]}_i}
= \frac{\partial \hat{y}_k}{\partial u_i}
= \frac{\partial \left( \frac{e ^ {u_k}}{A} \right)}{\partial u_i}
= \frac{\frac{\partial e ^ {u_k}}{\partial u_i} \times A  - \frac{\partial A}{\partial u_i} \times e ^ {u_k}}{A ^ 2}
= 0 - \left( \frac{e ^ {u_i} e ^ {u_k}}{A ^ {2}} \right)
= - \hat{y}_i \hat{y}_k
$
\end{latin}
\subsection{ج}
\begin{latin}
$
\frac{\partial L}{\partial z^{[2]}_i}
= \frac{\partial L}{\partial u_i}
= \frac{\partial \left( -\sum_{j = 1}^{K} y_j \log \hat{y}_j \right)}{\partial u_i}
= -\sum_{j = 1}^{K} y_j \times \frac{\partial \left( \log \hat{y}_j \right)}{\partial u_i}
= - 1 \times \frac{\partial \left( \log \hat{y}_k \right)}{\partial u_i}
= - \frac{\partial \left( \log \hat{y}_k \right)}{\partial \hat{y}_k} \times \frac{\partial \hat{y}_k}{\partial u_i}
= - \frac{1}{\hat{y}_k} \times \frac{\partial \hat{y}_k}{\partial u_i}
\\
= 
     \begin{cases}
       - \frac{1}{\hat{y}_k} \left( \hat{y}_k - \hat{y} ^ 2_k \right) &\quad i = k \\
       - \frac{1}{\hat{y}_k} \left( - \hat{y}_i \hat{y}_k \right) &\quad i \neq k \\
     \end{cases}
=
     \begin{cases}
       \hat{y}_k - 1 &\quad i = k \\
       \hat{y}_i &\quad i \neq k \\
     \end{cases}
$
\end{latin}
\subsection{د}

\subsection{ه}
مشکل این تابع در محاسبات عددی از آنجا آغاز می‌شود که اگر $z_i$ها اعداد بزرگی باشند، ممکن است شبکه با مشکلات پایداری عددی روبرو شود. از آنجایی که اعداد ممیز شناور محدودیت دارند، عملیات اعداد ممیز شناور با دقت محدودی انجام شود که ممکن است به کاهش دقت محاسبات منجر شود. این پدیده به عنوان "انفجار گرادیان" نیز شناخته می‌شود.

برای جلوگیری از \lr{overflow} باید مانع از زیاد بزرگ شدن اعداد شویم. البته \lr{underflow} هم در این حالت ممکن است رخ دهد و این ایده‌آل ما نیست؛ در این حالت مقادیر عبارات 0 می‌شوند. اما این بهتر از مقدار بی‌نهایت یا تعریف نشده است.

فرمول اصلاح شده به دلایل زیر می‌تواند به حل این مشکل کمک کند:
\begin{enumerate}
\item از آنجایی که
$z^{[2]}_i - m \le 0$
همه مقادیر
$e ^ {z ^ {[2]}_i - m}$
بین صفر و یک خواهند بود. این مشکل \lr{overflow} را برطرف می‌کند (البته مشکل \lr{underflow} سر جایش باقی است).
\item حداقل یکی از مقادیر نمایی برابر 1 است (در حالتی که
$z^{[2]}_i = m$
باشد). در این صورت این را تضمین می‌کند که حداقل یک مقدار دچار \lr{underflow} نمی‌شود. همچنین مخرج کسر همواره بزرگتر مساوی 1 است که از تقسیم بر صفر جلوگیری می‌کند. و در نهایت حداقل 1 مقدار غیر صفر در صورت کسر وجود دارد و \lr{softmax} نمی‌تواند یک بردار صفر خروجی دهد.
\end{enumerate}


%5
\section{}
\subsection{الف}
\begin{latin}
$
z = 
\begin{bmatrix}
12 & 14 & 14 & 12 \\
0 & 10 & 10 & 0 \\
-5 & 5 & 5 & -5
\end{bmatrix}
\\
\mu_1 = \frac{12+14+14+12}{4} = 13 \\
\mu_2 = \frac{0+10+10+0}{4} = 5 \\
\mu_3 = \frac{-5+5+5-5}{4} = 0 \\
\sigma_1 = \sqrt{\frac{\left( 12 - 13 \right) ^ 2 +\left( 14 - 13 \right) ^ 2 +\left( 14 - 13 \right) ^ 2 +\left( 12 - 13 \right) ^ 2}{4}} = 1 \\
\sigma_2 = \sqrt{\frac{\left( 0 - 5 \right) ^ 2 +\left( 10 - 5 \right) ^ 2 +\left( 10 - 5 \right) ^ 2 +\left( 0 - 5 \right) ^ 2}{4}} = 5 \\
\sigma_3 = \sqrt{\frac{\left( -5 - 0 \right) ^ 2 +\left( 5 - 0 \right) ^ 2 +\left( 5 - 0 \right) ^ 2 +\left( -5 - 0 \right) ^ 2}{4}} = 5 \\
\\ \\
z_{norm} = 
\begin{bmatrix}
\frac{1}{\sigma_1} & 0 & 0 \\
0 & \frac{1}{\sigma_2} & 0 \\
0 & 0 & \frac{1}{\sigma_3}
\end{bmatrix}
\left(
z - 
\begin{bmatrix}
\mu_1 & \mu_1 & \mu_1 & \mu_1 \\
\mu_2 & \mu_2 & \mu_2 & \mu_2 \\
\mu_3 & \mu_3 & \mu_3 & \mu_3
\end{bmatrix}
\right)
=
\begin{bmatrix}
-1 & 1 & 1 & -1 \\
-1 & 1 & 1 & -1 \\
-1 & 1 & 1 & -1
\end{bmatrix}
$
\end{latin}

\subsection{ب}
\begin{enumerate}
\item شبکه‌ها سریع‌تر آموزش می‌بینند: علی رغم اینکه هر تکرار آموزش به دلیل محاسبات اضافی در جریان \lr{forward} و پارامترهای اضافی برای آموزش در جریان \lr{backward} بسیار کندتر می‌شود، اما همگرایی بسیار سریع‌تر رخ می‌دهد و به طور کلی فرآیند آموزش سریع‌تر می‌شود.
\item امکان استفاده از نرخ‌های یادگیری بالاتر: نزول گرادیان معمولاً نیازمند نرخ‌های یادگیری کوچکتر برای همگرایی شبکه است و هرچه شبکه‌ها عمیق‌تر می‌شوند، در جریان \lr{backward} گرادیان‌های آنها کوچکتر می‌شوند. بنابراین نیاز به تکرارهای بیشتری دارند. استفاده از \lr{batch normalization} امکان استفاده از نرخ‌های یادگیری بسیار بالاتر را فراهم می‌کند، که سرعت آموزش شبکه‌ها را بهبود می‌بخشد.
\item سهولت در مقداردهی اولیه وزن‌ها: مقداردهی اولیه وزن‌ها ممکن است دشوار باشد و این مسئله در ساختارهای عمیق‌تر شدت می‌گیرد. \lr{batch normalization} به ما امکان می‌دهد که نسبت به انتخاب وزن‌های اولیه کمتر حساسیت کمتری داشته باشیم.
\item استفاده از توابع فعال‌سازی متنوع‌تر: برخی از توابع فعال‌سازی در برخی از مواقع به خوبی عمل نمی‌کنند. سیگموید‌ها به سرعت گرادیان خود را از دست می‌دهند. به این معنا که نمی‌توانند در شبکه‌های عمیق استفاده شوند. همچنین \lr{ReLU}ها اغلب در حین آموزش از بین می‌روند. به عبارتی کاملاً یادگیری متوقف شده است. بنابراین نیاز به توجه به بازه مقادیر وارد شده به آنها داریم. با تنظیم مقادیر وارد شده به هر تابع فعال‌سازی، \lr{batch normalization} باعث می‌شود که توابع غیرخطی که به نظر می‌آید در شبکه‌های عمیق خوب کار نمی‌کنند، به صورت مجدد قابل استفاده شوند.
\item سهولت در ساختاردهی شبکه‌های عمیق‌تر: به دلیل موارد گفته شده در بالا، ساختاردهی و آموزش شبکه‌های عصبی عمیق با استفاده از \lr{batch normalization} ساده‌تر و سریع‌تر است و نشان داده شده است که شبکه‌های عمیق به طور کلی نتایج بهتری ارائه می‌دهند.
\item \lr{regularization}: \lr{batch normalization} کمی نویز به شبکه اضافه می‌کند. در برخی موارد، مانند ماژول‌های \lr{Inception}، \lr{batch normalization} نیز همانند \lr{dropout} عمل می‌کند. اما به طور کلی، \lr{batch normalization} را به عنوان مقدار کمی \lr{regularization} اضافی در نظر بگیرید که شاید امکان کاهش برخی از \lr{dropout} را در یک شبکه فراهم کند.
\item به طور کلی می‌تواند نتایج بهتری ارائه دهد: برخی آزمایش‌ها نشان می‌دهند که \lr{batch normalization} به بهبود نتایج آموزش کمک می‌کند. با این حال این بهینه‌سازی به کمک آموزش سریع‌تر است، بنابراین نباید به آن به عنوان یک راه برای بهبود شبکه تلقی شود. اما از آنجا که اجازه می‌دهد تا شبکه‌ها را سریع‌تر آموزش بدهیم، این به این معناست که می‌توان به سرعت بیشتری روی طراحی‌های مختلف تست انجام بدهیم. همچنین این امکان را می‌دهد تا شبکه‌های عمیق‌تر بسازیم که به طور کلی بهتر هستند. پس احتمالاً نتایج بهتری خواهیم داشت.
\end{enumerate}
\subsection{ج}
\lr{Covariate shift}، وضعیتی است که در آن توزیع ویژگی‌های ورودی مدل در محیط تولید (تست) نسبت به آنچه مدل در طول آموزش و اعتبارسنجی دیده است تغییر می‌کند.
\subsection{د}




\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%1
\section{}
\subsection{الف}

توابع فعال‌سازی واحد خطی تصحیح شده \lr{(ReLU)} به چند دلیل به طور معمول در یادگیری عمیق بیشتر از توابع فعال‌سازی تانژانت هایپربولیک (\lr{tanh}) مورد استفاده قرار می‌گیرند:
\begin{enumerate}
\item مشکل محو شیب: توابع فعال‌سازی تانژانتی این خاصیت را دارند که مشتق آن‌ها در اطراف مقدار 0 بیشترین مقدار را دارد. این به معنای آن است که در هنگام بازگشتی به عقب (\lr{backpropagation}) ، زمانی که شیب محاسبه و به عقب از طریق شبکه منتقل می‌شود، شیب‌ها به اندازه‌ای کوچک می‌شوند که با افزایش مقدار مطلق ورودی، بسیار کوچک می‌شوند. این می‌تواند به مشکل محو شیب \lr{(vanishing gradient)} منجر شود که در آن لایه‌های اولیه یک شبکه عمیق مقدار به‌روزرسانی کمی دریافت می‌کنند و به سختی می‌توانند به طور مؤثر یاد بگیرند. از طرف دیگر، تابع \lr{ReLU} این مشکل محو شیب را ندارد زیرا مشتق آن برای ورودی‌های مثبت 1 است.

\item محاسبه ساده‌تر: تابع \lr{ReLU} از نظر محاسباتی ارزان‌تر از تابع \lr{tanh} است. محاسبه تابع \lr{tanh} شامل توان‌گیری و تقسیم است، که عملیات محاسباتی مقایسه شده با عملیات آستانه‌گذاری ساده تابع \lr{ReLU} بیشتر توان مصرف می‌کند. این باعث می‌شود که \lr{ReLU} موثرتر و سریعتر برای آموزش باشد.

\item \lr{sparsity} و غیرخطیت: واحدهای \lr{ReLU} می‌توانند \lr{sparsity} در شبکه ایجاد کنند. زیرا برای ورودی‌های منفی خروجی 0 می‌دهند. \lr{sparsity} در برخی موارد مفید است. زیرا شبکه را تشویق می‌کند که بر روی زیرمجموعه‌ای از ویژگی‌های ورودی تمرکز کند. علاوه بر این، واحدهای \lr{ReLU} غیرخطیت ارائه می‌دهند که برای مدل کردن توابع پیچیده توسط شبکه‌های عمیق مورد نیاز است.

\item موفقیت تجربی: توابع \lr{ReLU} در آموزش شبکه‌های عصبی عمیق موفقیت نشان داده‌اند. آن‌ها به طور گسترده در معماری‌های مختلف یادگیری عمیق مانند شبکه‌های عصبی کانوولوشنی (\lr{CNN}) و شبکه‌های عصبی مکرر (\lr{RNN}) به کار رفته‌اند و در مدل‌های برتر متعددی استفاده شده‌اند.
\end{enumerate}

با این حال، مهم است به یاد داشت که تابع \lr{ReLU} همراه با محدودیت‌های خود نیز هست. ممکن است مشکل "مرگ \lr{ReLU}" رخ دهد که در آن واحدهای \lr{ReLU} طی آموزش غیرفعال شوند (همیشه خروجی 0 دهند) و هیچگاه به حالت عادی باز نگردند. این مشکل با استفاده از نسخه‌های تغییریافته از \lr{ReLU} مانند \lr{Leaky ReLU} یا \lr{Parametric ReLU} قابل حل است که به واحدها امکان می‌دهد که برای ورودی‌های منفی شیبی کوچک داشته باشند و جلوی غیرفعال شدن کامل واحدها را بگیرند.

در عمل، انتخاب بین \lr{ReLU} و \lr{tanh} به مسئله خاص، معماری شبکه و سایر پارامترهای مدل بستگی دارد. در این مورد پاسخ یکتایی وجود ندارد و پژوهشگران اغلب با توابع فعال‌سازی مختلف آزمایش می‌کنند تا بیابند کدام تابع برای یک وظیفه خاص بهتر عمل می‌کند.

\subsection{ب}
یکی از مهم‌ترین موانع در آموزش شبکه‌های عصبی عمیق (\lr{DNN})، مشکل محو شیب است که در آن شیب‌ها (گرادیان‌ها) تابع هزینه نسبت به وزن‌های لایه‌های اولیه به‌طور قابل‌توجهی کوچک می‌شوند. به عبارت دیگر، لایه‌های اولیه کمتر یا اصلاً اطلاعات وزن به‌روزشده‌ای در هنگام بازگشت به عقب (\lr{backpropagation}) دریافت می‌کنند که به همگرایی کند یا حتی رکود منتج می‌شود. مشکل محو شیب به‌طور اصلی به انتخاب توابع فعال‌ساز و روش‌های بهینه‌سازی در شبکه‌های عصبی عمیق مرتبط است.

روابط مشتق دو تابع به همراه نمودار مشتق آن‌ها رسم شده است.

\begin{latin}
\subsubsection{\lr{ReLU}}
$
ReLU(x) = x ^ + = \max(0, x)\\
ReLU'(x) =
\begin{cases}
0, & \text{if } x < 0 \\
1, & \text{if } x \geq 0
\end{cases}
$
\subsubsection{\lr{Sigmoid}}
$
\sigma(x) = \frac{1}{1 + e ^ {-x}} \\
\sigma'(x) = \sigma(x) \left( 1 - \sigma(x) \right)
$
%%%%%%%%%
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$x$},
    ylabel={$f'(x)$},
    legend style={at={(0.95,0.95)},anchor=north east},
]

% ReLU derivative
\addplot[blue, domain=-2:2, samples=100] {x > 0 ? 1 : 0};
\addlegendentry{ReLU}

% Sigmoid derivative
\addplot[red, domain=-3:3, samples=100] {exp(-x)/(1+exp(-x))^2};
\addlegendentry{Sigmoid}

\end{axis}
\end{tikzpicture}
\caption{Derivatives of ReLU and Sigmoid functions.}
\end{figure}
%%%%%%%%%
\end{latin}


شیب توابع فعال‌ساز سیگمویدی به طور معمول به سرعت محو می‌شوند. یک بازه نسبتاً کوچکی از ورودی‌ها وجود دارد که مشتق تابع سیگموید به میزان کافی غیرصفر است. به عبارت دیگر، هنگامی که سیگموید به یکی از دو قله چپ یا راست می‌رسد، تقریباً بی‌معنی است که یک مرور به عقب از آن انجام داده شود، زیرا مشتق به صفر میل می‌کند. از سوی دیگر، تابع \lr{ReLU} تنها هنگامی اشباع می‌شود که ورودی کمتر از صفر باشد. حتی این اشباع می‌تواند با استفاده از واحدهای \lr{Leaky ReLU} مهار شود. در شبکه‌های عمیق، اشباع یادگیری را مختل می‌کند، بنابراین \lr{ReLU} می‌تواند راه‌حل مناسب‌تری نسبت به سیگموید باشد. البته باید به یاد داشت که با توجه به نوع داده و شرایط مسئله هر یک این توابع می‌تواند عملکرد بهتری نسبت به دیگری داشته باشد.
\subsection{ج}

وقتی مقدار اولیه وزن‌ها در یک شبکه عصبی با تابع فعال‌سازی \lr{Sigmoid} بسیار بزرگ باشد، چند مشکل ممکن است پیش بیاید:
\begin{enumerate}
\item    اشباع شبکه: ورودی‌های بزرگ باعث می‌شود تابع فعال‌سازی \lr{Sigmoid} به سرعت به 1 (یا به سمت صفر) اشباع شود. این به این معناست که توابع فعال‌سازی \lr{Sigmoid} به سرعت مقدار‌های خروجی نزدیک به 1 (یا 0) تولید می‌کنند، و این می‌تواند به مشکل شبکه اصلی شما بیافزاید، زیرا گرادیان‌ها به سرعت به صفر نزدیک می‌شوند.

\item    مشکل محو شیب: همچنین، وقتی مقدار وزن‌ها بسیار بزرگ باشد، مشکل محو شیب (\lr{vanishing gradient}) نیز ممکن است به وجود بیاید. زیرا مشتق تابع \lr{Sigmoid} در نزدیکی نقطه میانی (0.5) بیشینه می‌شود و با افزایش فاصله از این نقطه در هر دو جهت، مشتق به سرعت به صفر نزدیک می‌شود. این موجب می‌شود که گرادیان‌ها بسیار کوچک شوند و به شبکه کمکی در آموزش نکنند.

\item    سختی آموزش: شبکه‌های عصبی با تابع فعال‌سازی \lr{Sigmoid} و وزن‌های بزرگ ممکن است به سرعت به مسئله آموزش نشدنی تبدیل شوند. آموزش شبکه‌های ژرف با این ویژگی‌ها به طور کلی سخت‌تر است و نیازمند تنظیمات و حل مشکلات ویژه‌ای می‌باشد.
\end{enumerate}

برای مقابله با این مشکلات، می‌توان از مقادیر اولیه وزن مناسب‌تری استفاده کرد، مانند مقادیر تصادفی کوچکتر یا از روش‌های مانند نرمالیزه کردن وزن‌ها بهره برد. همچنین، ممکن است از توابع فعال‌سازی دیگری که بهتر با مقادیر بزرگ کار می‌کنند، مانند \lr{ReLU} یا \lr{Leaky ReLU}، استفاده کرد. انتخاب تابع فعال‌سازی و مقادیر اولیه وزن‌ها باید به توجه به کاربرد و معماری خاص شبکه انجام شود.


%2
\section{}
\subsection{الف}
در شبکه‌های عصبی، استفاده از توابع فعال‌ساز غیرخطی به دلایل متعددی ضروری است:
\begin{enumerate}
\item    قابلیت نمایش توابع پیچیده: توابع فعال‌ساز غیرخطی امکان نمایش توابع پیچیده‌تری را فراهم می‌کنند. اگر از توابع خطی استفاده شود (مثل تابع همانی)، شبکه توانایی نمایش توابع پیچیده و غیرخطی را نخواهد داشت. توابع غیرخطی می‌توانند الگوهای پیچیده‌تر و ساختارهای عمیق‌تر را مدل کنند.

\item    اهمیت انطباق به داده: توابع غیرخطی به شبکه‌های عصبی اجازه می‌دهند که بهتر به داده‌ها انطباق پیدا کنند. این به معنای این است که توابع فعال‌سازی غیرخطی می‌توانند بر اساس ویژگی‌های داده و نمونه‌ها، تغییر کنند و توانایی شبکه را در تطبیق بهتر با تغییرات در داده ارتقا می‌دهند.

\item    مدل‌سازی تعاملات غیرخطی: در بسیاری از کاربردها، ارتباطات و تعاملات بین متغیرها و ویژگی‌های ورودی غیرخطی هستند. توابع فعال‌سازی غیرخطی به شبکه‌ها امکان می‌دهند تا تعاملات غیرخطی را به خوبی مدل کنند و از توانایی شبکه در پیش‌بینی تعاملات پیچیده بهره‌برند.

\item    مشکل محو شیب را کاهش می‌دهند: توابع غیرخطی معمولاً مشکل محو شیب را کاهش می‌دهند. توابع خطی به سرعت به صفر همگرا می‌شوند و مشتق‌های کوچکی دارند، اما توابع فعال‌سازی غیرخطی (مانند \lr{ReLU}) مشتق‌های بزرگ‌تری دارند و این به شبکه‌ها امکان می‌دهد که در دوره‌های آموزشی عمیق‌تر پایدارتر باشند.
\end{enumerate}

با توجه به این دلایل، توابع فعال‌سازی غیرخطی مهمی در موفقیت شبکه‌های عصبی در مدل‌سازی و پیش‌بینی وظایف پیچیده و غیرخطی ایفا می‌کنند.

\subsection{ب}
\begin{latin}
$
g\left( x \right) = - \min\left( 5, x \right)\\
h\left( x \right) =
\begin{cases}
\max\left( x, 0.3x \right), & x \geq 0\\
\min\left( x, 0.3x \right), & x < 0
\end{cases} \\ \overset{\text{if } x \geq 0: \:\: x \geq 0.3x}{\underset{\text{if } x < 0: \:\: x < 0.3x}{\Longrightarrow }}
h\left( x \right) = x
$

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel={$x$},
    ylabel={$y$},
]

% Plot of g(x) = -min(5, x) in blue
\addplot[blue, domain=-10:15, samples=100] {-min(5, x)};
\addlegendentry{g(x)}

% Plot of h(x) = x in red
\addplot[red, domain=-10:15, samples=100] {x};
\addlegendentry{h(x)}

\end{axis}
\end{tikzpicture}
\caption{Plot of $g(x) = -\min(5, x)$ and $h(x) = x$}
\end{figure}
\end{latin}
توابع فعال‌ساز نقش حیاتی در شبکه‌های عصبی عمیق دارند و تأثیر زیادی بر توانایی شبکه در یادگیری و حل مسائل مختلف دارند. انتخاب توابع فعال‌ساز می‌تواند به طور قابل توجهی بر آموزش و عملکرد شبکه تأثیر بگذارد.

\subsubsection{$g(x)$}

این تابع حداقل مقدار بین $x$ و 5 را می‌گیرد، آن را منفی می‌کند و نتیجه را به عنوان خروجی استفاده می‌کند. این تابع می‌تواند به عنوان تابع فعال‌سازی استفاده شود، اما مشکلاتی دارد:
\begin{enumerate}
\item    اشباع: این تابع برای مقادیر $x > 5$ صاف می‌شود و گرادیان آن در این ناحیه صفر است. این می‌تواند منجر به محو شدن گرادیان‌ها در هنگام \lr{backpropagation} شود، که باعث دشوار شدن آموزش شبکه می‌شود.

\item    عدم غیرخطیت: توابع فعال‌سازی باید غیرخطیت را به وارد شبکه کنند. $g(x)$ یک تابع قطعه قطعه خطی است که تنها یک نقطه انحراف در $x = 5$ دارد. این کمبود غیرخطیت می‌تواند توانایی شبکه در مدل کردن روابط پیچیده در داده‌ها را محدود کند.

\item    عدم محبوبیت: این تابع در عمل برای شبکه‌های عصبی عمیق به صورت معمول مورد استفاده قرار نمی‌گیرد، و توابع فعال‌سازی معتبرتری وجود دارند که به خوبی کار می‌کنند، مانند \lr{ReLU} و نسخه‌های مشتق شده از آن.

\end{enumerate}

\subsubsection{$h(x)$}

این تابع یک تابع فعال‌سازی خطی است، یعنی غیرخطیتی را به شبکه معرفی نمی‌کند. اگرچه ممکن است در برخی معماری‌های شبکه‌های عصبی (مانند رگرسیون خطی) مورد استفاده قرار گیرد، اما به طور کلی در شبکه‌های عصبی عمیق به عنوان تابع فعال‌سازی استفاده نمی‌شود، به علت دلایل متعددی از جمله:
\begin{enumerate}
\item    ظرفیت مدل‌سازی محدود: شبکه‌های عصبی عمیق از توابع فعال‌سازی غیرخطی برای مدل‌سازی روابط غیرخطی و پیچیده در داده‌ها استفاده می‌کنند. استفاده از $h(x) = x$ در تمام شبکه تقریباً شبیه به یک مدل خطی تک لایه می‌شود و توانایی نمایش الگوهای پیچیده را محدود می‌کند.
\end{enumerate}

%3
\section{}
\subsection{الف}
تابع تانژانت هایپربولیک ($tanh$) اغلب به عنوان نسخه مقیاس‌شده‌ای از تابع سیگموید توصیف می‌شود، به خصوص تابع سیگموید لجستیک. این رابطه به دلیل شباهت‌های تابع تانژانت و تابع سیگموید وجود دارد، اما در بازه و مقیاس‌شان تفاوت دارند.\\
تابع سیگموید که اغلب با نماد $\sigma\left( x \right)$ نشان داده می‌شود، به شرح زیر تعریف می‌شود:
\begin{latin}
$
\sigma\left( x \right) = \frac{1}{1 + e ^ {-x}}
$
\end{latin}
این تابع هر عدد حقیقی را به یک مقدار بین 0 و 1 نگاشت می‌کند. وقتی $x$ یک عدد مثبت بزرگ است، $\sigma\left( x \right)$ به 1 نزدیک می‌شود و وقتی $x$ یک عدد منفی بزرگ است، $\sigma\left( x \right)$ به 0 نزدیک می‌شود. این به این معناست که تابع سیگموید ورودی خود را در بازه (0، 1) فشرده می‌کند که برای مسائل دسته‌بندی دودویی مفید است، چون می‌توان از آن تعبیر احتمالاتی کرد.\\
تابع تانژانت هایپربولیک، به شکل زیر تعریف می‌شود:
\begin{latin}
$
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e ^ {2x} - 1}{e ^ {2x} + 1}
$
\end{latin}
تابع تانژانت هایپربولیک هر عدد حقیقی را به یک مقدار بین 1- و 1 نگاشت می‌دهد. وقتی $x$ یک عدد مثبت بزرگ است، $tanh(x)$ به 1 نزدیک می‌شود و وقتی $x$ یک عدد منفی بزرگ است، $tanh(x)$ به 1- نزدیک می‌شود.\\

رابطه بین توابع تانژانت هایپربولیک و سیگموید به شرح زیر است:
\begin{enumerate}
\item مقیاس‌دهی: تابع تانژانت هایپربولیک، انتقال داده شده و مقیاس شده‌ی تابع سیگموید به منظور داشتن رنج $(-1, 1)$ به جای $(0, 1)$ است. این مقیاس‌دهی با کم کردن $0.5$ از تابع سیگموید و سپس ضرب آن در 2 انجام می‌شود:
\begin{latin}
$
2\sigma\left( 2x \right)-1 = 
2 \times \frac{1}{1 + e ^ {-2x}} - 1 =
\frac{1 - e ^ {-2x}}{1 + e ^ {-2x}} = 
\frac{1 - e ^ {-2x}}{1 + e ^ {-2x}} \times \frac{e ^ {2x}}{e ^ {2x}} =
\frac{e ^ {2x} - 1}{e ^ {2x} + 1} = tanh\left( x \right)\\ \Rightarrow 
tanh(x) = 2\sigma\left( 2x \right) - 1
$
\end{latin}

\item تقارن: یکی از تفاوت‌های کلیدی این است که تابع تانژانت هایپربولیک در اطراف مبدأ متقارن است (در واقع $tanh(-x) = -tanh(x)$)، درحالی که تابع سیگموید این تقارن را ندارد.

\end{enumerate}
تابع تانژانت هایپربولیک اغلب به عنوان یک جایگزین برای تابع سیگموید در شبکه‌های عصبی استفاده می‌شود. زیرا به دلیل ماهیت مرکز شده حول صفری که دارد، یادگیری در شبکه‌های عمیق را سریع‌تر می‌کند. اینکه تابع تانژانت هایپربولیک مقادیر منفی را نیز خروجی دهد به معنای این است که می‌تواند تغییرات مثبت و منفی را در واحدهای مخفی شبکه در طول آموزش ایجاد کند که می‌تواند به همگرایی کمک کند. با این حال، هر دو تابع هنوز در متنوعی از زمینه‌ها استفاده می‌شوند و انتخاب بین آن‌ها بستگی به مسئله خاص و معماری شبکه دارد.



\subsection{ب}
\begin{latin}
$
p\left( x \right) = x \log\left( 1 + tanh\left( e ^ {x} \right) \right)\\ \\
\frac{d}{dx} p\left( x \right) = \left( \frac{d}{dx}x \right) \times \log\left( 1 + tanh\left( e ^ {x} \right) \right) +
\left( \frac{d}{dx}\left( \log\left( 1 + tanh\left( e ^ {x} \right) \right) \right) \right) \times x \\=
\log\left( 1 + tanh\left( e ^ {x} \right) \right) + 
x\left( \frac{d }{dx}\left( 1 + tanh\left( e ^ {x} \right) \right) \right) \times \frac{1}{1 + tanh\left( e ^ {x} \right)}
\\= \log\left( 1 + tanh\left( e ^ {x} \right) \right) +
\frac{x}{1 + tanh\left( e ^ {x} \right)} \times \left( \frac{d }{dx}\left( tanh\left( e ^ {x} \right) \right) \right)
\\= \log\left( 1 + tanh\left( e ^ {x} \right) \right) + \frac{x}{1 + tanh\left( e ^ {x} \right)} \times 
\left( \frac{d }{dx}\left( e ^ {x} \right) \right) \times \left( 1 - tanh^2\left( e ^ x \right) \right)
\\=
\log\left( 1 + tanh\left( e ^ {x} \right) \right) + 
xe^x\left( 1 - tanh\left( e ^ x \right) \right)
$
\end{latin}

%4
\section{}
\begin{latin}
\begin{neuralnetwork}[height=9]
    \newcommand{\x}[2]{$x_#2$}
    \newcommand{\y}[2]{$\hat{y}_#2$}
    \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
    \newcommand{\hsecond}[2]{\small $h^{(2)}_#2$}
    \inputlayer[count=4, bias=true, title=Input\\layer, text=\x]
    \hiddenlayer[count=7, bias=true, title=Hidden\\layer, text=\hfirst] \linklayers
    \outputlayer[count=3, title=Output\\layer, text=\y] \linklayers
\end{neuralnetwork}
\end{latin}
با توجه به شبکه‌ی بالا، نورون‌های سبز، بنفش، و قرمز به ترتیب لایه‌ی ورودی، لایه‌ی مخفی، و لایه‌ی خروجی را تشکیل می‌دهند و همچنین نورون‌های زرد \lr{bias}ها هستند که همگی مقدار 1 دارند. پارامترهای قابل یادگیری شبکه وزن‌های موجود بین نورون‌هاست که تعدادشان برابر است با:
$
4 \times 7 + 7 + 7 \times 3 + 3 = 59
$


%5
\section{}

\begin{latin}
\begin{neuralnetwork}[height=4, layerspacing=20mm, nodespacing=15mm, maintitleheight=2.5em, layertitleheight=2.5em]
    \newcommand{\x}[2]{$x_#2$}
    \newcommand{\relu}[2]{\small $a_#2$}
    \newcommand{\y}[2]{$\hat{y}$}
    \inputlayer[count=2, bias=false, title=Input Layer, text=\x]
    \hiddenlayer[count=3, bias=false, title=ReLU Layer, text=\relu] \linklayers
    \outputlayer[count=1, title=Output Layer, text=\y] \linklayers
\end{neuralnetwork}
\end{latin}

\begin{latin}
$
J\left( W \right) = \frac{1}{n} \sum_{i = 1}^{n} L\left( \hat{y} ^ {( i )}, y ^ { ( i ) } \right) = \left( \hat{y} ^ {( 1 )} - y ^ {( 1 )} \right) ^ {2} = \left( \hat{y} - 3 \right) ^ {2}
\\
x = \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\\
z = \begin{bmatrix}
z_1 \\
z_2 \\
z_3
\end{bmatrix} =
\begin{bmatrix}
w_1 & w_2 \\
w_3 & w_4 \\
w_5 & w_6
\end{bmatrix}
x
\\
a = \begin{bmatrix}
a_1 \\
a_2 \\
a_3
\end{bmatrix} =
ReLU\left( z \right)
\\
z_4 =
\begin{bmatrix}
w_7 &
w_8 &
w_9
\end{bmatrix}a
\\
\hat{y} = ReLU\left( z_4 \right)
\\
\\
\\
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}=
\begin{bmatrix}
1 \\
2
\end{bmatrix}
\\ y = 3
\\
\begin{bmatrix}
z_1 \\
z_2 \\
z_3
\end{bmatrix} =
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
\begin{bmatrix}
1 \\
2
\end{bmatrix} =
\begin{bmatrix}
4 \\
-3 \\
5
\end{bmatrix}
\\ a = 
ReLU\left(
\begin{bmatrix}
4 \\
-3 \\
5
\end{bmatrix}
\right) =
\begin{bmatrix}
4 \\
0 \\
5
\end{bmatrix}
\\
z_4 = 
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
\begin{bmatrix}
4 \\
0 \\
5
\end{bmatrix} = 6
\\
\hat{y} = ReLU\left( 6 \right) = 6
\\
\\
\\
\begin{bmatrix}
w_7 &
w_8 &
w_9
\end{bmatrix}
\gets 
\begin{bmatrix}
w_7 &
w_8 &
w_9
\end{bmatrix}
-\alpha
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial w_7} &
\frac{\partial J\left( W \right)}{\partial w_8} &
\frac{\partial J\left( W \right)}{\partial w_9}
\end{bmatrix} 
\\ =
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
-0.1
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_7} &
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_8} &
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial w_9}
\end{bmatrix}
\\=
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
-0.1
\begin{bmatrix}
2\left( \hat{y} - y \right)a_1 &
2\left( \hat{y} - y \right)a_2 &
2\left( \hat{y} - y \right)a_3
\end{bmatrix}
\\=
\begin{bmatrix}
-1 &
3 &
2
\end{bmatrix}
-0.1
\begin{bmatrix}
24 &
0 &
30
\end{bmatrix} = \begin{bmatrix}
-3.4 &
3 &
-1
\end{bmatrix}
\\ \\ 
\begin{bmatrix}
w_1 & w_2 \\
w_3 & w_4 \\
w_5 & w_6
\end{bmatrix}
\gets 
\begin{bmatrix}
w_1 & w_2 \\
w_3 & w_4 \\
w_5 & w_6
\end{bmatrix}
-\alpha
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial w_1} & \frac{\partial J\left( W \right)}{\partial w_2} \\
\frac{\partial J\left( W \right)}{\partial w_3} & \frac{\partial J\left( W \right)}{\partial w_4} \\
\frac{\partial J\left( W \right)}{\partial w_5} & \frac{\partial J\left( W \right)}{\partial w_6}
\end{bmatrix}
\\=
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
-0.1
\begin{bmatrix}
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}
&
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_2}
\\
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_2} \times \frac{\partial a_2}{\partial z_2} \times \frac{\partial z_2}{\partial w_3}
&
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_2} \times \frac{\partial a_2}{\partial z_2} \times \frac{\partial z_2}{\partial w_4}
\\
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_3} \times \frac{\partial a_3}{\partial z_3} \times \frac{\partial z_3}{\partial w_5}
&
\frac{\partial J\left( W \right)}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial a_3} \times \frac{\partial a_3}{\partial z_3} \times \frac{\partial z_3}{\partial w_6}
\end{bmatrix}
\\=
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
-0.1
\begin{bmatrix}
2\left( \hat{y} - y \right) \times w_7 \times 1 \times x_1
&
2\left( \hat{y} - y \right) \times w_7 \times 1 \times x_2
\\
2\left( \hat{y} - y \right) \times w_8 \times 0 \times x_1
&
2\left( \hat{y} - y \right) \times w_8 \times 0 \times x_2
\\
2\left( \hat{y} - y \right) \times w_9 \times 1 \times x_1
&
2\left( \hat{y} - y \right) \times w_9 \times 1 \times x_2
\end{bmatrix}
\\=
\begin{bmatrix}
2 & 1 \\
1 & -2 \\
1 & 2
\end{bmatrix}
-0.1
\begin{bmatrix}
-6
&
-12
\\
0
&
0
\\
12
&
36
\end{bmatrix} =
\begin{bmatrix}
2.6 & 2.2 \\
1 & -2 \\
-0.2 & -1.6
\end{bmatrix}
$
\end{latin}


%6
\section{}
\subsection{الف}
\begin{latin}


\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,433); %set diagram left start at 0, and has height of 433

%Shape: Rectangle [id:dp26082320028943795] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (62,46) -- (170.07,46) -- (170.07,94) -- (62,94) -- cycle ;
%Straight Lines [id:da5321728940522126] 
\draw    (22.33,21.8) -- (60.29,44.96) ;
\draw [shift={(62,46)}, rotate = 211.39] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da8722522064456113] 
\draw    (26.07,70.2) -- (60,70.01) ;
\draw [shift={(62,70)}, rotate = 179.68] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4977170317452869] 
\draw    (24.33,113.8) -- (60.23,94.93) ;
\draw [shift={(62,94)}, rotate = 152.27] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da9786960106080183] 
\draw    (171,70.8) -- (201.07,70.99) ;
\draw [shift={(203.07,71)}, rotate = 180.36] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp4699732294865028] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (203,46) -- (317.07,46) -- (317.07,94) -- (203,94) -- cycle ;
%Shape: Rectangle [id:dp7233773663060797] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (109,155) -- (228.33,155) -- (228.33,203) -- (109,203) -- cycle ;
%Straight Lines [id:da5907930980218636] 
\draw    (28.33,179.8) -- (107,179.02) ;
\draw [shift={(109,179)}, rotate = 179.43] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da5935250653594418] 
\draw    (27.33,227.8) -- (107.09,203.58) ;
\draw [shift={(109,203)}, rotate = 163.11] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Straight Lines [id:da4807719277385877] 
\draw    (228.33,179.8) -- (276.07,179.99) ;
\draw [shift={(278.07,180)}, rotate = 180.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp004733566676465184] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (278,155) -- (392.07,155) -- (392.07,203) -- (278,203) -- cycle ;
%Curve Lines [id:da43051210766963555] 
\draw    (316.83,78.1) .. controls (498.83,114.1) and (-103.67,122.8) .. (109,155) ;
\draw [shift={(109,155)}, rotate = 188.61] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp7844392260191501] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (183,265) -- (280.33,265) -- (280.33,313) -- (183,313) -- cycle ;
%Straight Lines [id:da7438844634576284] 
\draw    (280.33,289.8) -- (350.07,289.99) ;
\draw [shift={(352.07,290)}, rotate = 180.16] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp06514973867064988] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (352,265) -- (438.33,265) -- (438.33,313) -- (352,313) -- cycle ;
%Curve Lines [id:da5234327252277915] 
\draw    (316.83,63.1) .. controls (853.83,299.1) and (-37.87,212.1) .. (183,265) ;
\draw [shift={(183,265)}, rotate = 193.47] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da12663621077553533] 
\draw    (392.83,178.1) .. controls (593.17,244.9) and (-122.67,181.8) .. (183,313) ;
\draw [shift={(183,313)}, rotate = 203.23] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp1997427400696722] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (237,364) -- (554.43,364) -- (554.43,412) -- (237,412) -- cycle ;
%Straight Lines [id:da05813852175573886] 
\draw    (25.33,411.8) -- (235,412) ;
\draw [shift={(237,412)}, rotate = 180.05] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Curve Lines [id:da6379195786923225] 
\draw    (438.43,290.1) .. controls (707.43,328.1) and (-151.57,360.1) .. (237,364) ;
\draw [shift={(237,364)}, rotate = 180.58] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

% Text Node
\draw (67,60) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle z_{1} =w_{1} x+b_{1}$};
% Text Node
\draw (7,11) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle x$};
% Text Node
\draw (3,60) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle w_{1}$};
% Text Node
\draw (5,107) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b_{1}$};
% Text Node
\draw (208,60) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a_{1} =ReLU( z_{1})$};
% Text Node
\draw (114,169) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle z_{2} =w_{2} a_{1} +b_{2}$};
% Text Node
\draw (4,170) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle w_{2}$};
% Text Node
\draw (5,217) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle b_{2}$};
% Text Node
\draw (283,169) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle a_{2} =ReLU( z_{2})$};
% Text Node
\draw (188,279) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle z_{3} =a_{1}^{2} +a_{2}^{2}$};
% Text Node
\draw (357,279) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \hat{y} \ =\ \sigma ( z_{3})$};
% Text Node
\draw (242,378) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle \mathcal{L}(\hat{y} \ ,\ y) \ =-[ y\ log\ \hat{y} \ +( 1-y) \ log\ ( 1-\hat{y} \ )] \ $};
% Text Node
\draw (6,401) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\displaystyle y$};


\end{tikzpicture}

\end{latin}


\subsection{ب}
\begin{latin}
$
\frac{\partial L}{\partial a_1} =
\frac{\partial L}{\partial \hat{y}} \times
\frac{\partial \hat{y}}{\partial z_3} \times
\frac{\partial z_3}{\partial a_1} =
\left(\frac{-y}{\hat{y}} + \frac{1-y}{1-\hat{y}}\right) \times
\hat{y}\left( 1-\hat{y} \right) \times
2a_1 =
\left( \hat{y} - y \right)2a_1
$
\\
\\
$
\frac{\partial L}{\partial a_2} =
\frac{\partial L}{\partial \hat{y}} \times
\frac{\partial \hat{y}}{\partial z_3} \times
\frac{\partial z_3}{\partial a_2} =
\left(\frac{-y}{\hat{y}} + \frac{1-y}{1-\hat{y}}\right) \times
\hat{y}\left( 1-\hat{y} \right) \times
2a_2 =
\left( \hat{y} - y \right)2a_2
$
\\
\\
$
\frac{\partial L}{\partial w_1} =
\frac{\partial L}{\partial \hat{y}} \times
\frac{\partial \hat{y}}{\partial z_3} \times
\left(
\frac{\partial z_3}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}            +
\frac{\partial z_3}{\partial a_2} \times \frac{\partial a_2}{\partial z_2} \times \frac{\partial z_2}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}
\right)
\\=
\frac{\partial L}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}
+
\frac{\partial L}{\partial a_2} \times \frac{\partial a_2}{\partial z_2} \times \frac{\partial z_2}{\partial a_1} \times \frac{\partial a_1}{\partial z_1} \times \frac{\partial z_1}{\partial w_1}
\\=
\left( \hat{y} - y \right)2a_1 \times a_1\left( 1 - a_1 \right) \times x
+
\left( \hat{y} - y \right)2a_2 \times a_2\left( 1 - a_2 \right) \times w_2 \times a_1\left( 1 - a_1 \right) \times x
\\=
2\left( \hat{y} - y \right)a_1\left( 1 - a_1 \right)x\left[ a_1 + a_2 ^ 2\left( 1 - a_2 \right) w_2 \right]
$
\\
\\
$
\frac{\partial L}{\partial w_2} =
\frac{\partial L}{\partial \hat{y}} \times
\frac{\partial \hat{y}}{\partial z_3} \times
\frac{\partial z_3}{\partial a_2} \times
\frac{\partial a_2}{\partial z_2} \times
\frac{\partial z_2}{\partial w_2} 
\\=
\frac{\partial L}{\partial a_2} \times
\frac{\partial a_2}{\partial z_2} \times
\frac{\partial z_2}{\partial w_2} =
\left( \hat{y} - y \right)2a_2 \times a_2\left( 1 - a_2 \right) \times a_1
$
\end{latin}


%\begin{enumerate}
%	\item \lr{Bob} لیستی از الگوریتم‌هایی که از آن‌ها پشتیبانی می‌کند را به همراه \lr{nonce} به \lr{Alice} می‌فرستد.
%	\item \lr{Alice} بین لیست الگوریتم‌های پیشنهادی یکی را انتخاب می‌کند و آن را به همراه \lr{certificate} و \lr{nonce} می‌فرستد.
%	\item \lr{Bob} \lr{certificate} را اعتبارسنجی می‌کند، \lr{public key}ِ \lr{Alice} را استخراج می‌کند، \lr{pre\_master\_secret} را تولید می‌کند، با \lr{public key}ِ \lr{Alice} آن را رمز می‌کند و برای \lr{Alice} می‌فرستد.
%	\item \lr{Bob} و \lr{Alice} به طور مستقل رمز و کلیدهای \lr{MAC} را از \lr{pre\_master\_secret} و \lr{nonce}ها محاسبه می‌کنند.
%	\item \lr{Bob} یک \lr{MAC} از همه‌ی پیام‌های \lr{handshake} می‌فرستد.
%	\item \lr{Alice} یک \lr{MAC} از همه‌ی پیام‌های \lr{handshake} می‌فرستد.
%\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section*{منابع}
\renewcommand{\section}[2]{}%
\begin{thebibliography}{99} % assumes less than 100 references
%چنانچه مرجع فارسی نیز داشته باشید باید دستور فوق را فعال کنید و مراجع فارسی خود را بعد از این دستور وارد کنید


\begin{LTRitems}

\resetlatinfont

\bibitem{b1} https://jaykmody.com/blog/stable-softmax/

\bibitem{b1} https://towardsdatascience.com/batch-normalization-8a2e585775c9

\end{LTRitems}

\end{thebibliography}


\end{document}
